{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextAugment\n",
    "TextAugment est une bibliothèque Python 3 permettant d'augmenter le texte pour les applications de traitement du langage naturel. TextAugment repose sur les épaules géantes de NLTK, Gensim et TextBlob et joue bien avec eux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\samir\\anaconda3\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\samir\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: gensim in c:\\users\\samir\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\samir\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Collecting googletrans\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "     -------------------------------------- 55.1/55.1 kB 124.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (2021.10.8)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2022.8.1-py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 17.8 kB/s eta 0:00:00\n",
      "Collecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "     --------------------------------------- 58.8/58.8 kB 79.8 kB/s eta 0:00:00\n",
      "Collecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "     --------------------------------------- 42.6/42.6 kB 21.8 kB/s eta 0:00:00\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ------------------------------------- 133.4/133.4 kB 18.4 kB/s eta 0:00:00\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.2.0)\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "     --------------------------------------- 65.0/65.0 kB 14.5 kB/s eta 0:00:00\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "     -------------------------------------- 53.6/53.6 kB 106.6 kB/s eta 0:00:00\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\samir\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15734 sha256=6fae605c548dae9644a60c6f2895d5c4c88231d8c9fa2da445cb5ce251316cd5\n",
      "  Stored in directory: c:\\users\\samir\\appdata\\local\\pip\\cache\\wheels\\27\\f3\\32\\d4859d40071f07a5df0ab6fdc0076e78a8a786625dde2b4b2f\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.2\n",
      "    Uninstalling idna-3.2:\n",
      "      Successfully uninstalled idna-3.2\n",
      "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.8.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy nltk gensim textblob googletrans\n",
    "\n",
    "\"\"\"Gensim :\n",
    "is a free open-source Python library for representing documents as semantic vectors\"\"\"\n",
    "\n",
    "\"\"\" TextBlob : \n",
    "is a Python (2 and 3) library for processing textual data.\n",
    "It provides a simple API for diving into common natural language processing (NLP) tasks \n",
    "such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, \n",
    "translation, and more.\"\"\"\n",
    "\n",
    "\"\"\"Googletrans :\n",
    "is a free and unlimited python library that implemented Google Translate API. \n",
    "This uses the Google Translate Ajax API to make calls to such methods as detect and translate.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textaugment\n",
      "  Downloading textaugment-1.3.4-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: googletrans in c:\\users\\samir\\anaconda3\\lib\\site-packages (from textaugment) (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\samir\\anaconda3\\lib\\site-packages (from textaugment) (1.22.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\samir\\anaconda3\\lib\\site-packages (from textaugment) (4.2.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\samir\\anaconda3\\lib\\site-packages (from textaugment) (0.17.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\samir\\anaconda3\\lib\\site-packages (from textaugment) (3.6.5)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim->textaugment) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim->textaugment) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from gensim->textaugment) (1.7.1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from googletrans->textaugment) (0.13.3)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (1.5.0)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2022.8.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2021.10.8)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (2.10)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (0.9.1)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (3.0.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans->textaugment) (1.2.0)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\samir\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans->textaugment) (3.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk->textaugment) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk->textaugment) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk->textaugment) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samir\\anaconda3\\lib\\site-packages (from nltk->textaugment) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\samir\\anaconda3\\lib\\site-packages (from click->nltk->textaugment) (0.4.4)\n",
      "Installing collected packages: textaugment\n",
      "Successfully installed textaugment-1.3.4\n"
     ]
    }
   ],
   "source": [
    "! pip install textaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\samir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\"\"\"WordNet :\n",
    "est une base de données lexicale\n",
    "Son but est de répertorier, classifier et mettre en relation de diverses manières le contenu \n",
    "sémantique et lexical de la langue anglaise \n",
    "utilisé à plusieurs buts :\n",
    "    la désambiguïsation du sens des mots, \n",
    "    la recherche d'informations, \n",
    "    la classification automatique des textes,\n",
    "    le résumé automatique des textes, \n",
    "    la traduction automatique \n",
    "    la génération automatique de mots croisés.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" punkt :\n",
    "Ce tokenizer divise un texte en une liste de phrases en utilisant un algorithme non supervisé \n",
    "pour créer un modèle pour les mots d'abréviation, \n",
    "les collocations et les mots qui commencent des phrases.\"\"\"\n",
    "\n",
    "\"\"\"averaged_perceptron_tagger :\n",
    "punkt est utilisé pour 'tokenising' les phrases \n",
    "et averaged_perceptron_tagger est utilisé pour étiqueter les mots avec leurs parties du discours (POS)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textaugment import Wordnet\n",
    "\n",
    "t = Wordnet()\n",
    "\n",
    "TEST_TEXT = \"The quick brown fox jumps over the lazy dog.\"\n",
    "t.augment(TEST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown dodger jumps over the lazy dog.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textaugment import Wordnet\n",
    "v = True # enable verbs augmentation. By default is True.\n",
    "n = False # enable nouns augmentation. By default is False.\n",
    "#runs = 1 # number of times to augment a sentence. By default is 1.\n",
    "p = 0.5 # The probability of success of an individual trial. (0.1<p<1.0), default is 0.5. Used by Geometric distribution to selects words from a sentence.\n",
    "\n",
    "t_with_args = Wordnet(v=False ,n=True, p=0.9)\n",
    "t_with_args.augment(TEST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA: Techniques simples d'augmentation des données\n",
    "1. Remplacement de synonyme\n",
    "2. Suppression aléatoire\n",
    "3. Échange aléatoire\n",
    "4. Insertion aléatoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remplacement de synonyme\n",
    "Choisissez au hasard n mots de la phrase qui ne sont pas des mots vides. Remplacez chacun de ces mots par un de ses synonymes choisis au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL Text : The quick brown fox jumps over the lazy dog.\n",
      "AUGMENTED TEXT: The immediate brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "from textaugment import EDA\n",
    "\n",
    "eda_augment = EDA()\n",
    "\n",
    "TEST_TEXT = \"The quick brown fox jumps over the lazy dog.\"\n",
    "syn_augmented = eda_augment.synonym_replacement(TEST_TEXT)\n",
    "print(\"ORIGINAL Text :\",TEST_TEXT)\n",
    "print(\"AUGMENTED TEXT:\", syn_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression aléatoire\n",
    "Supprimez au hasard chaque mot de la phrase avec probabilité p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL Text : The quick brown fox jumps over the lazy dog.\n",
      "AUGMENTED TEXT: quick brown fox jumps over the lazy\n"
     ]
    }
   ],
   "source": [
    "from textaugment import EDA\n",
    "\n",
    "eda_augment = EDA()\n",
    "\n",
    "TEST_TEXT = \"The quick brown fox jumps over the lazy dog.\"\n",
    "random_del_augmented = eda_augment.random_deletion(TEST_TEXT, p=0.2)\n",
    "print(\"ORIGINAL Text :\",TEST_TEXT)\n",
    "print(\"AUGMENTED TEXT:\", random_del_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Échange aléatoire\n",
    "Choisissez au hasard deux mots dans la phrase et échangez leurs positions. Faites cela n fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL Text : The quick brown fox jumps over the lazy dog.\n",
      "AUGMENTED TEXT: The brown quick fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "from textaugment import EDA\n",
    "\n",
    "eda_augment = EDA()\n",
    "\n",
    "TEST_TEXT = \"The quick brown fox jumps over the lazy dog.\"\n",
    "random_swap_augmented = eda_augment.random_swap(TEST_TEXT)\n",
    "print(\"ORIGINAL Text :\",TEST_TEXT)\n",
    "print(\"AUGMENTED TEXT:\", random_swap_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion aléatoire\n",
    "Trouvez un synonyme aléatoire d'un mot aléatoire dans la phrase qui n'est pas un mot vide. Insérez ce synonyme dans une position aléatoire dans la phrase. Faites ceci n fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL Text : The quick brown fox jumps over the lazy dog.\n",
      "AUGMENTED TEXT: The ready quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "from textaugment import EDA\n",
    "\n",
    "eda_augment = EDA()\n",
    "\n",
    "TEST_TEXT = \"The quick brown fox jumps over the lazy dog.\"\n",
    "rnd_insert_augmented = eda_augment.random_insertion(TEST_TEXT)\n",
    "print(\"ORIGINAL Text :\",TEST_TEXT)\n",
    "print(\"AUGMENTED TEXT:\", rnd_insert_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "85ab11090a32b58d1dba88bd22dc30d9d16814845c03675a7b99eed0df35fc67"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
